# -*- coding: utf-8 -*-
"""Project-Code2-Team5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14hfTfCYzrfV9hpbP_0JjoxUTIJXA8rBI
"""

# prompt: CONNECT TO MY DRIVE

from google.colab import drive
drive.mount('/content/drive')

# prompt: LOAD DATA FROM MY DRIVE
import pandas as pd
df_train = pd.read_csv('//content/drive/MyDrive/AIML/ProjectTrainingDataBalanced.csv')
df_train.head()

df_train.shape



X = df_train.drop(['click'], axis=1)
y = df_train['click']

from sklearn.model_selection import train_test_split

# Split the data - 80% train, 20% test, for example
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import log_loss, accuracy_score

# Define your model with GPU support
model = XGBClassifier(
    use_label_encoder=False
    #tree_method='gpu_hist',      # Use GPU histogram algorithm
    #predictor='gpu_predictor'    # Use GPU for prediction
    #device = 'cuda'             # Use GPU for training
)

# Define the parameter grid
param_grid = {
    'n_estimators': [100,150],
    'max_depth': [12, 15, 18],
    'learning_rate': [0.01, 0.1, 0.2],
    # Add other parameters as needed
}

# Initialize GridSearchCV
grid_search = GridSearchCV(model, param_grid, cv=2, scoring='neg_log_loss', verbose=1, n_jobs=-1)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Best parameters and score
print("Best parameters found: ", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Predictions and Evaluation
probabilities = best_model.predict_proba(X_test)
loss = log_loss(y_test, probabilities)
print(f'Log Loss: {loss}')

# predictions = best_model.predict(X_test)
# accuracy = accuracy_score(y_test, predictions)
# print(f'Accuracy: {accuracy}')

best_model

df_test = pd.read_csv('//content/drive/MyDrive/AIML/ProjectTestingEncoded.csv')
df_test.shape

df_test.shape

probabilities1 = best_model.predict_proba(df_test)[:,1]

len(probabilities1)

len(probabilities1[probabilities1 > 0.5])

df_subm = pd.read_csv('//content/drive/MyDrive/AIML/ProjectSubmission-TeamX.csv')
df_subm.head()

df_subm.shape

# columns = [ 'P(click)']
# df1 = pd.DataFrame(probabilities1, columns=columns)
# df1.shape

df_subm['P(click)'] =probabilities1
df_subm.head()

df_subm.to_csv('//content/drive/MyDrive/AIML/ProjectSubmission-Team5.csv', index=False)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(random_state=42)
param_grid = {
    'n_estimators': [20, 22, 25],  # Number of trees in the forest
    'max_depth': [20, 25, 30],        # Maximum depth of the tree
    # Add other parameters as needed
}


grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_log_loss', verbose=2, n_jobs=-1)

 # Load your training data here
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)

from sklearn.metrics import log_loss, accuracy_score
best_model1 = grid_search.best_estimator_
# You can now use best_model for predictions or further analysis
# Predictions and Evaluation
probabilities2 = best_model1.predict_proba(X_test)
loss = log_loss(y_test, probabilities2)
print(f'Log Loss: {loss}')

best_model

